# -*- coding: utf-8 -*-
"""IMDB_SentimentAnalysisandRecommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ENXH7zissNh9NIqpaR3fChKc02zkjTe
"""

import os
import json
from zipfile import ZipFile
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

kaggle_dictionary=json.load(open('kaggle.json'))

os.environ['KAGGLE_USERNAME']=kaggle_dictionary['username']
os.environ['KAGGLE_KEY']=kaggle_dictionary['key']

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!ls

with ZipFile("imdb-dataset-of-50k-movie-reviews.zip",'r') as zip_ref:
  zip_ref.extractall()

data=pd.read_csv('/content/IMDB Dataset.csv')

data.shape

"""we have to make sure class distribution is even so that there is no data imbalance."""

data['sentiment'].value_counts()

data.replace({'sentiment':{'positive':1, 'negative':0}}, inplace=True)

train_data, test_data=train_test_split(data, test_size=0.2, random_state=42)

tokenizer=Tokenizer(num_words=5000)

tokenizer.fit_on_texts(train_data['review'])

X_train=pad_sequences(tokenizer.texts_to_sequences(train_data['review']),maxlen=200)
X_test=pad_sequences(tokenizer.texts_to_sequences(test_data['review']),maxlen=200)

Y_train=train_data['sentiment']
Y_test=test_data['sentiment']

model=Sequential()
model.add(Embedding(input_dim=5000,output_dim=128,input_length=200))
model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='accuracy', patience=0, verbose=1, baseline=None, mode='auto')

history=model.fit(X_train,Y_train,epochs=5,batch_size=64, validation_split=0.2,callbacks=early_stopping)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

def predict_sentiment(review):
  sequence = tokenizer.texts_to_sequences([review])
  padded_sequence = pad_sequences(sequence, maxlen=200)
  prediction = model.predict(padded_sequence)
  sentiment = "positive" if prediction[0][0] > 0.5 else "negative"
  return sentiment

rev='I could not have hated this movie more'
sent=predict_sentiment(rev)
print(sent)

rev='I could not have loved this movie more'
sent=predict_sentiment(rev)
print(sent)

!pip install langchain langchain_openai

import os
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"]="sk-proj-JBySFtSR7UnfTMyAj1cxT3BlbkFJa9Ab7G4iEo6VcPlPVaQR"

llm=ChatOpenAI(model="gpt-3.5", temperature=0)

from langchain_core.prompts import ChatPromptTemplate

prompt=ChatPromptTemplate.from_messages(
    [
        ("system","You are a helpful assistant that recommends five different movies based on user review."),
        ("human","{input}")])

chain= prompt | llm
response=chain.invoke({"input": rev})

response.content